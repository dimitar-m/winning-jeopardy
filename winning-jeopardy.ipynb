{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd813a8-ecb7-4a5a-84a5-e46b95874b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d79f1-44b7-47dc-b48d-9b9dd6875068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06a51985-3e1f-44ce-9480-674725f1de22",
   "metadata": {},
   "source": [
    "## Introduction  \n",
    "\n",
    "This project analyzes the full Jeopardy dataset to uncover patterns that might provide a competitive edge in the game. The main objective is to investigate whether specific words in the question text can serve as subtle hints for the correct answers more often than expected by chance.  \n",
    "\n",
    "To explore this, we cleaned and preprocessed the dataset, then applied statistical tests like the chi-squared test to compare word frequency distributions between the training and test sets. The focus was on identifying words that frequently appear alongside particular answers, potentially revealing useful patterns for players.  \n",
    "\n",
    "The analysis identified several statistically significant terms—such as *art*, *shows*, and *baseball*—that appeared more often in questions related to their corresponding answers. These findings suggest that recognizing such patterns could help improve a player’s guessing accuracy, especially when similar categories reappear in the game.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758a96e-6e48-4ee7-845a-029059156dad",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "---\n",
    "1. [Loading and Exploring the Jeopardy Dataset](#Loading-and-Exploring-the-Jeopardy-Dataset)  \n",
    "2. [Data Cleaning and Preprocessing](#Data-Cleaning-and-Preprocessing)  \n",
    "3. [Answer-Question Word Overlap](#Answer-Question-Word-Overlap)  \n",
    "4. [Question Reuse in Jeopardy](#Question-Reuse-in-Jeopardy)  \n",
    "5. [Word Distribution in Jeopardy Questions](#Word-Distribution-in-Jeopardy-Questions)  \n",
    "6. [Conclusion](#Conclusion)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd7f20e-6e56-4d04-9f96-562da8d80c71",
   "metadata": {},
   "source": [
    "## Loading and Exploring the Jeopardy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6d5d09-3706-4adf-920f-25641579d6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Overview:\n",
      "==========================================================================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 216930 entries, 0 to 216929\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   category     216930 non-null  object\n",
      " 1   air_date     216930 non-null  object\n",
      " 2   question     216930 non-null  object\n",
      " 3   value        213296 non-null  object\n",
      " 4   answer       216930 non-null  object\n",
      " 5   round        216930 non-null  object\n",
      " 6   show_number  216930 non-null  int64 \n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 11.6+ MB\n",
      "None\n",
      "\n",
      "First few rows of the dataset:\n",
      "==========================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>air_date</th>\n",
       "      <th>question</th>\n",
       "      <th>value</th>\n",
       "      <th>answer</th>\n",
       "      <th>round</th>\n",
       "      <th>show_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HISTORY</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'For the last 8 years of his life, Galileo was...</td>\n",
       "      <td>$200</td>\n",
       "      <td>Copernicus</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'No. 2: 1912 Olympian; football star at Carlis...</td>\n",
       "      <td>$200</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'The city of Yuma in this state has a record a...</td>\n",
       "      <td>$200</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'In 1963, live on \"The Art Linkletter Show\", t...</td>\n",
       "      <td>$200</td>\n",
       "      <td>McDonald\\'s</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'Signer of the Dec. of Indep., framer of the C...</td>\n",
       "      <td>$200</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3-LETTER WORDS</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'In the title of an Aesop fable, this insect s...</td>\n",
       "      <td>$200</td>\n",
       "      <td>the ant</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HISTORY</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'Built in 312 B.C. to link Rome &amp; the South of...</td>\n",
       "      <td>$400</td>\n",
       "      <td>the Appian Way</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'No. 8: 30 steals for the Birmingham Barons; 2...</td>\n",
       "      <td>$400</td>\n",
       "      <td>Michael Jordan</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'In the winter of 1971-72, a record 1,122 inch...</td>\n",
       "      <td>$400</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'This housewares store was named for the packa...</td>\n",
       "      <td>$400</td>\n",
       "      <td>Crate &amp; Barrel</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          category    air_date  \\\n",
       "0                          HISTORY  2004-12-31   \n",
       "1  ESPN's TOP 10 ALL-TIME ATHLETES  2004-12-31   \n",
       "2      EVERYBODY TALKS ABOUT IT...  2004-12-31   \n",
       "3                 THE COMPANY LINE  2004-12-31   \n",
       "4              EPITAPHS & TRIBUTES  2004-12-31   \n",
       "5                   3-LETTER WORDS  2004-12-31   \n",
       "6                          HISTORY  2004-12-31   \n",
       "7  ESPN's TOP 10 ALL-TIME ATHLETES  2004-12-31   \n",
       "8      EVERYBODY TALKS ABOUT IT...  2004-12-31   \n",
       "9                 THE COMPANY LINE  2004-12-31   \n",
       "\n",
       "                                            question value          answer  \\\n",
       "0  'For the last 8 years of his life, Galileo was...  $200      Copernicus   \n",
       "1  'No. 2: 1912 Olympian; football star at Carlis...  $200      Jim Thorpe   \n",
       "2  'The city of Yuma in this state has a record a...  $200         Arizona   \n",
       "3  'In 1963, live on \"The Art Linkletter Show\", t...  $200     McDonald\\'s   \n",
       "4  'Signer of the Dec. of Indep., framer of the C...  $200      John Adams   \n",
       "5  'In the title of an Aesop fable, this insect s...  $200         the ant   \n",
       "6  'Built in 312 B.C. to link Rome & the South of...  $400  the Appian Way   \n",
       "7  'No. 8: 30 steals for the Birmingham Barons; 2...  $400  Michael Jordan   \n",
       "8  'In the winter of 1971-72, a record 1,122 inch...  $400      Washington   \n",
       "9  'This housewares store was named for the packa...  $400  Crate & Barrel   \n",
       "\n",
       "       round  show_number  \n",
       "0  Jeopardy!         4680  \n",
       "1  Jeopardy!         4680  \n",
       "2  Jeopardy!         4680  \n",
       "3  Jeopardy!         4680  \n",
       "4  Jeopardy!         4680  \n",
       "5  Jeopardy!         4680  \n",
       "6  Jeopardy!         4680  \n",
       "7  Jeopardy!         4680  \n",
       "8  Jeopardy!         4680  \n",
       "9  Jeopardy!         4680  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values:\n",
      "==========================================================================================================================\n",
      "category          0\n",
      "air_date          0\n",
      "question          0\n",
      "value          3634\n",
      "answer            0\n",
      "round             0\n",
      "show_number       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the JSON file into a DataFrame\n",
    "raw_data = pd.read_json(\"JEOPARDY_QUESTIONS1.json\")\n",
    "\n",
    "def explore(df):\n",
    "    '''\n",
    "    Function to explore datasets:\n",
    "    - Display information about the DataFrame\n",
    "    - Show the first few rows\n",
    "    - Show the null values for each column\n",
    "    '''\n",
    "    print('Data Overview:\\n', '='*122, sep='')\n",
    "    print(df.info())  # Displays the structure of the DataFrame: column types, counts, and memory usage.\n",
    "    \n",
    "    print('\\nFirst few rows of the dataset:\\n', '='*122, sep='')\n",
    "    display(df.head(10))  # Display 10 rows for a quick glimpse of the dataset structure and content.\n",
    "    \n",
    "    print('\\nNull values:\\n', '='*122, sep='')\n",
    "    print(df.isnull().sum())  # Counts missing values in each column to assess data quality.\n",
    "         \n",
    "# Call the explore function to perform initial data inspection.\n",
    "explore(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7ee283-9e67-471f-baed-288ebf195801",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bcbbab5-bd4d-4ada-8be8-7f51de403b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated column names:\n",
      "======================\n",
      "Index(['category', 'air_date', 'question', 'value_usd', 'answer', 'round',\n",
      "       'show_number'],\n",
      "      dtype='object')\n",
      "\n",
      "Updated data types:\n",
      "======================\n",
      "category               object\n",
      "air_date       datetime64[ns]\n",
      "question               object\n",
      "value_usd               Int64\n",
      "answer                 object\n",
      "round                  object\n",
      "show_number             int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import janitor\n",
    "\n",
    "# Clean column names\n",
    "jeopardy_df = raw_data.clean_names(strip_underscores=True)\n",
    "\n",
    "# Convert 'air_date' to datetime format\n",
    "jeopardy_df[\"air_date\"] = pd.to_datetime(jeopardy_df[\"air_date\"])\n",
    "\n",
    "# Rename 'value' to 'value_usd' and clean its values\n",
    "jeopardy_df = jeopardy_df.rename(columns={\"value\": \"value_usd\"})\n",
    "jeopardy_df[\"value_usd\"] = (\n",
    "    jeopardy_df[\"value_usd\"]\n",
    "    .str.replace(\"$\", \"\", regex=False)  # Remove dollar sign\n",
    "    .str.replace(\",\", \"\", regex=False)  # Remove commas\n",
    "    .replace(\"None\", None)  # Handle missing values\n",
    "    .astype(\"float\")  # Convert to float first\n",
    "    .fillna(0)  # Replace NaN values with 0\n",
    "    .astype(\"Int64\")  # Convert to integer (allows nullable Int64)\n",
    ")\n",
    "\n",
    "# Display the updates\n",
    "\n",
    "# Display updated column names\n",
    "print('Updated column names:\\n', '='*22, '\\n', jeopardy_df.columns, sep='')\n",
    "\n",
    "# Display data types after conversion\n",
    "print('\\nUpdated data types:\\n', '='*22, '\\n', jeopardy_df.dtypes, sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89ba4b19-c4de-4bc8-b30f-0fbf3af3c20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>air_date</th>\n",
       "      <th>question</th>\n",
       "      <th>value_usd</th>\n",
       "      <th>answer</th>\n",
       "      <th>round</th>\n",
       "      <th>show_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HISTORY</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'For the last 8 years of his life, Galileo was...</td>\n",
       "      <td>200</td>\n",
       "      <td>Copernicus</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'No. 2: 1912 Olympian; football star at Carlis...</td>\n",
       "      <td>200</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'The city of Yuma in this state has a record a...</td>\n",
       "      <td>200</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'In 1963, live on \"The Art Linkletter Show\", t...</td>\n",
       "      <td>200</td>\n",
       "      <td>McDonald\\'s</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'Signer of the Dec. of Indep., framer of the C...</td>\n",
       "      <td>200</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          category   air_date  \\\n",
       "0                          HISTORY 2004-12-31   \n",
       "1  ESPN's TOP 10 ALL-TIME ATHLETES 2004-12-31   \n",
       "2      EVERYBODY TALKS ABOUT IT... 2004-12-31   \n",
       "3                 THE COMPANY LINE 2004-12-31   \n",
       "4              EPITAPHS & TRIBUTES 2004-12-31   \n",
       "\n",
       "                                            question  value_usd       answer  \\\n",
       "0  'For the last 8 years of his life, Galileo was...        200   Copernicus   \n",
       "1  'No. 2: 1912 Olympian; football star at Carlis...        200   Jim Thorpe   \n",
       "2  'The city of Yuma in this state has a record a...        200      Arizona   \n",
       "3  'In 1963, live on \"The Art Linkletter Show\", t...        200  McDonald\\'s   \n",
       "4  'Signer of the Dec. of Indep., framer of the C...        200   John Adams   \n",
       "\n",
       "       round  show_number  \n",
       "0  Jeopardy!         4680  \n",
       "1  Jeopardy!         4680  \n",
       "2  Jeopardy!         4680  \n",
       "3  Jeopardy!         4680  \n",
       "4  Jeopardy!         4680  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e763ffec-7544-4c23-8fcf-86e067d2d1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     for the last 8 years of his life  galileo was...\n",
      "1     no  2  1912 olympian  football star at carlis...\n",
      "2     the city of yuma in this state has a record a...\n",
      "3     in 1963  live on  the art linkletter show   t...\n",
      "4     signer of the dec  of indep   framer of the c...\n",
      "Name: clean_question, dtype: object\n",
      "0     copernicus\n",
      "1     jim thorpe\n",
      "2        arizona\n",
      "3    mcdonald  s\n",
      "4     john adams\n",
      "Name: clean_answer, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to normalize text\n",
    "def normalize_text_column(series):\n",
    "    \"\"\"Takes a Pandas Series and normalizes text by:\n",
    "       - Converting to lowercase\n",
    "       - Replacing non-word characters with spaces\n",
    "    \"\"\"\n",
    "    return series.str.replace(r'\\W', ' ', regex=True).str.lower()\n",
    "\n",
    "# Apply normalization to the Question and Answer columns\n",
    "jeopardy_df[\"clean_question\"] = normalize_text_column(jeopardy_df[\"question\"])\n",
    "jeopardy_df[\"clean_answer\"] = normalize_text_column(jeopardy_df[\"answer\"])\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(jeopardy_df['clean_question'].head())\n",
    "print(jeopardy_df['clean_answer'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4f532d-0777-46c4-b024-4f08e2c1aa64",
   "metadata": {},
   "source": [
    "## Data Cleaning & Preprocessing Steps\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Cleaning Column Names  \n",
    "The dataset contained leading spaces in some column names. To standardize them, we used the `clean_names()` method from the `janitor` library, which:  \n",
    "- Converted column names to **lowercase**.  \n",
    "- Removed **leading/trailing spaces**.  \n",
    "- Replaced spaces with **underscores**.  \n",
    "- Ensured names **did not start with an underscore**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Converting Data Types  \n",
    "\n",
    "#### **Air Date Column**  \n",
    "- Originally stored as a **string**.  \n",
    "- Converted to **datetime** format for easier time-based analysis.  \n",
    "\n",
    "#### **Value Column (`value_usd`)**  \n",
    "- Renamed from `value` to `value_usd` for clarity.  \n",
    "- Removed **`$` symbols** and **commas**.  \n",
    "- Converted from **string** to **integer** (`Int64`).  \n",
    "- Replaced missing values (`<NA>`) with `0` to avoid issues in calculations.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Normalizing Text Data  \n",
    "To ensure consistency in text-based analysis, we normalized the **Question** and **Answer** columns by:  \n",
    "- Converting all text to **lowercase**.  \n",
    "- Removing **non-word characters** (punctuation, special symbols, etc.).  \n",
    "- Storing the cleaned versions in new columns: **`clean_question`** and **`clean_answer`**.  \n",
    "\n",
    "---\n",
    "\n",
    "### Final Notes  \n",
    "These preprocessing steps ensure that the dataset is well-structured and ready for deeper analysis, such as identifying patterns in Jeopardy questions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9508d99-cc13-4167-b8a6-8d6a81f3a0ba",
   "metadata": {},
   "source": [
    "## Filtering Non-informative Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "208b0ac1-b72d-4627-a169-e2431b721ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure stopwords are available\n",
    "# nltk.download(\"stopwords\") - execute the first time only\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Compute word frequency in all questions\n",
    "all_words = \" \".join(jeopardy_df[\"clean_question\"]).split()\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Find words occurring in more than 5% of questions\n",
    "threshold = 0.05 * len(jeopardy_df)\n",
    "common_words = {word for word, count in word_counts.items() if count > threshold}\n",
    "\n",
    "# Final set of words to remove (stopwords + overly common words)\n",
    "words_to_remove = stop_words | common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd0d6fc-855d-485b-950a-d6bb5f50721d",
   "metadata": {},
   "source": [
    "## Filtering Non-Informative Words in Jeopardy Questions  \n",
    "\n",
    "To enhance our analysis, we refine the dataset by removing non-informative words using natural language processing (NLP) techniques rather than relying on arbitrary length-based filters.  \n",
    "\n",
    "---\n",
    "\n",
    "### Approach  \n",
    "\n",
    "1. **Removing Standard Stopwords**  \n",
    "   - We use the `stopwords` list from the Natural Language Toolkit (NLTK) to filter out common English words like *the*, *and*, *is*, etc.  \n",
    "\n",
    "2. **Eliminating Overly Frequent Words**  \n",
    "   - We compute word frequency across all questions using the `Counter` class.  \n",
    "   - Any word appearing in more than **5% of all questions** is considered too common and removed.  \n",
    "\n",
    "3. **Creating a Comprehensive Filter List**  \n",
    "   - We combine standard stopwords with our computed list of frequent words.  \n",
    "   - This ensures only meaningful words remain for further analysis.  \n",
    "\n",
    "---\n",
    "\n",
    "### Outcome  \n",
    "\n",
    "By applying this filtering strategy, we retain only the most informative words, improving the quality of our analysis and ensuring a more focused approach to studying Jeopardy questions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb354676-e96a-441d-b325-3c4a01d9037c",
   "metadata": {},
   "source": [
    "## Answer-Question Word Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20e91668-a0f0-47f3-8faf-8dd83a5bcfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of answer_in_question: 0.06073353229966505\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate word overlap between answer and question\n",
    "def answer_in_question(row):\n",
    "    split_answer = set(row[\"clean_answer\"].split()) - {\"the\"}\n",
    "    split_question = set(row[\"clean_question\"].split())\n",
    "    \n",
    "    if not split_answer:\n",
    "        return 0  # Avoid division by zero\n",
    "    \n",
    "    match_count = len(split_answer & split_question)  # Set intersection for efficiency\n",
    "    return match_count / len(split_answer)\n",
    "\n",
    "# Apply the function to each row\n",
    "jeopardy_df[\"answer_in_question\"] = jeopardy_df.apply(answer_in_question, axis=1)\n",
    "answer_in_question_mean = jeopardy_df[\"answer_in_question\"].mean()\n",
    "print(\"Mean of answer_in_question:\", answer_in_question_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587efa49-1c50-4d2b-9803-a573520f0286",
   "metadata": {},
   "source": [
    "## Understanding Answer-Question Word Overlap  \n",
    "\n",
    "The average value of the `answer_in_question` column is **0.0607**, meaning that, on average, only **6.1%** of the words in an answer also appear in the corresponding question.  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights for Studying Strategy:  \n",
    "- **Low direct word overlap**: Jeopardy answers **rarely** appear verbatim in the questions.  \n",
    "- **Memorization alone isn’t enough**: Simply recalling common answers is not as effective as understanding **how** questions are phrased.  \n",
    "- **Focus on patterns and connections**: Studying **synonyms, related terms, and conceptual links** between questions and answers may yield better results.  \n",
    "- **Practice with past questions**: Identifying recurring themes and phrasing patterns can enhance recall and improve response accuracy.  \n",
    "\n",
    "By shifting from pure memorization to recognizing language patterns and conceptual relationships, we can develop a more effective Jeopardy study strategy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0894f8-da6b-45fc-b375-b3ebc8cdfda0",
   "metadata": {},
   "source": [
    "## Question Reuse in Jeopardy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3923b3be-3d30-4225-90ee-50ea5545432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of question_overlap: 0.944935790594037\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty list and set\n",
    "question_overlap = []\n",
    "terms_used = set()\n",
    "\n",
    "# Sort by air date\n",
    "jeopardy_df = jeopardy_df.sort_values(\"air_date\", ascending=True)\n",
    "\n",
    "# Iterate through each row\n",
    "for _, row in jeopardy_df.iterrows():\n",
    "    split_question = set(row[\"clean_question\"].split()) - words_to_remove  # Remove stopwords and common words\n",
    "    \n",
    "    match_count = len(split_question & terms_used)  # Count overlapping words\n",
    "    \n",
    "    terms_used.update(split_question)  # Add new words to terms_used\n",
    "    \n",
    "    question_overlap.append(match_count / len(split_question) if split_question else 0)\n",
    "\n",
    "# Assign results to the DataFrame\n",
    "jeopardy_df[\"question_overlap\"] = question_overlap\n",
    "question_overlap_mean = jeopardy_df[\"question_overlap\"].mean()\n",
    "print(\"Mean of question_overlap:\", question_overlap_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d55033d-9b9b-4fc0-b136-1ea7680b67bf",
   "metadata": {},
   "source": [
    "## Investigating Question Reuse in Jeopardy  \n",
    "\n",
    "The average **question_overlap** is approximately **0.9449**, meaning that nearly **95% of the significant words in a new Jeopardy question have appeared in previous questions**.  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights:  \n",
    "\n",
    "- **Frequent Term Reuse**: Many Jeopardy questions include words that have appeared in past questions, making familiarity with Jeopardy’s vocabulary a useful study strategy.  \n",
    "- **Recurring Topics**: Certain subjects and themes tend to resurface, even if the phrasing changes.  \n",
    "- **Effective Study Approach**: Focusing on commonly used terms, historical figures, scientific concepts, and frequently appearing topics can be beneficial.  \n",
    "- **Pattern Recognition**: Since question wording often overlaps, practicing with past questions can help in identifying how similar topics are framed in different contexts.  \n",
    "\n",
    "---\n",
    "\n",
    "### Summary  \n",
    "\n",
    "The high **question_overlap** score indicates that Jeopardy regularly recycles key terms across episodes. Reviewing past questions and familiarizing yourself with commonly used words could provide a significant advantage in predicting and answering future questions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9669fd1a-254d-4b45-8442-293eb7f6ee9e",
   "metadata": {},
   "source": [
    "## Word Distribution in Jeopardy Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c14ffd89-9a09-4a6e-a32e-92aaa3dbf1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: eastwood, Chi-Squared: 0.9996, p-value: 0.3174\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: norse, Chi-Squared: 0.4570, p-value: 0.4990\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: normally, Chi-Squared: 0.4006, p-value: 0.5268\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: know, Chi-Squared: 0.0026, p-value: 0.9597\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: shot, Chi-Squared: 0.0000, p-value: 1.0000\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: flesh, Chi-Squared: 0.3131, p-value: 0.5758\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: immigrants, Chi-Squared: 1.8589, p-value: 0.1728\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: revolt, Chi-Squared: 0.7192, p-value: 0.3964\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: 59, Chi-Squared: 0.1337, p-value: 0.7146\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: norman, Chi-Squared: 0.3086, p-value: 0.5785\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: veggie, Chi-Squared: 0.2323, p-value: 0.6298\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: sweet, Chi-Squared: 1.3877, p-value: 0.2388\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: ross, Chi-Squared: 0.0000, p-value: 1.0000\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: long, Chi-Squared: 1.9549, p-value: 0.1621\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: stamp, Chi-Squared: 2.2649, p-value: 0.1323\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: generation, Chi-Squared: 0.1124, p-value: 0.7375\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: briefly, Chi-Squared: 1.6334, p-value: 0.2012\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: boris, Chi-Squared: 0.0849, p-value: 0.7708\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: wilson, Chi-Squared: 0.0728, p-value: 0.7874\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: art, Chi-Squared: 34.3343, p-value: 0.0000\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: i, Chi-Squared: 28.5618, p-value: 0.0000\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: bergman, Chi-Squared: 7.1407, p-value: 0.0075\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: 1, Chi-Squared: 24.9779, p-value: 0.0000\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: titled, Chi-Squared: 6.9368, p-value: 0.0084\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: shrine, Chi-Squared: 0.0343, p-value: 0.8531\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: includes, Chi-Squared: 0.9488, p-value: 0.3300\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: gary, Chi-Squared: 0.1439, p-value: 0.7044\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: slam, Chi-Squared: 0.0000, p-value: 1.0000\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: fisher, Chi-Squared: 2.1752, p-value: 0.1402\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: billionaire, Chi-Squared: 1.0097, p-value: 0.3150\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: telephone, Chi-Squared: 1.6334, p-value: 0.2012\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: valentine, Chi-Squared: 0.0343, p-value: 0.8531\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: diet, Chi-Squared: 0.5487, p-value: 0.4589\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: hawaii, Chi-Squared: 0.0000, p-value: 1.0000\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: present, Chi-Squared: 0.0009, p-value: 0.9756\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: bunch, Chi-Squared: 0.1962, p-value: 0.6578\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: michigan, Chi-Squared: 1.0612, p-value: 0.3029\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: smell, Chi-Squared: 0.3300, p-value: 0.5657\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: 24, Chi-Squared: 0.1767, p-value: 0.6743\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: isn, Chi-Squared: 5.3276, p-value: 0.0210\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: street, Chi-Squared: 5.8760, p-value: 0.0153\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: chart, Chi-Squared: 1.1022, p-value: 0.2938\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: shows, Chi-Squared: 179.0947, p-value: 0.0000\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: reduce, Chi-Squared: 0.8010, p-value: 0.3708\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: armed, Chi-Squared: 0.9022, p-value: 0.3422\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: roosevelt, Chi-Squared: 0.3767, p-value: 0.5394\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: rooms, Chi-Squared: 0.0356, p-value: 0.8504\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: union, Chi-Squared: 1.2288, p-value: 0.2676\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: forces, Chi-Squared: 3.8238, p-value: 0.0505\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: why, Chi-Squared: 2.5670, p-value: 0.1091\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: positive, Chi-Squared: 11.0929, p-value: 0.0009\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: bars, Chi-Squared: 0.0567, p-value: 0.8118\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: royalty, Chi-Squared: 0.8420, p-value: 0.3588\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: nuts, Chi-Squared: 0.0826, p-value: 0.7738\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: commonly, Chi-Squared: 3.7585, p-value: 0.0525\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: 1849, Chi-Squared: 0.3271, p-value: 0.5674\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: neither, Chi-Squared: 0.2545, p-value: 0.6139\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: fantasy, Chi-Squared: 0.0196, p-value: 0.8887\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: doomed, Chi-Squared: 15.9259, p-value: 0.0001\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: tyler, Chi-Squared: 0.0000, p-value: 1.0000\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: sell, Chi-Squared: 0.2103, p-value: 0.6465\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: lift, Chi-Squared: 1.1950, p-value: 0.2743\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: away, Chi-Squared: 0.0050, p-value: 0.9434\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: leader, Chi-Squared: 10.3383, p-value: 0.0013\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: cool, Chi-Squared: 1.2685, p-value: 0.2600\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: philip, Chi-Squared: 3.0996, p-value: 0.0783\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: amazing, Chi-Squared: 0.0527, p-value: 0.8184\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: retirement, Chi-Squared: 0.1400, p-value: 0.7082\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: lowest, Chi-Squared: 0.4658, p-value: 0.4949\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: search, Chi-Squared: 4.5256, p-value: 0.0334\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: highlight, Chi-Squared: 0.1214, p-value: 0.7276\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: half, Chi-Squared: 0.3288, p-value: 0.5664\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: crowned, Chi-Squared: 0.0953, p-value: 0.7576\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: baseball, Chi-Squared: 12.1305, p-value: 0.0005\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: congressman, Chi-Squared: 1.4891, p-value: 0.2224\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: buchanan, Chi-Squared: 0.0111, p-value: 0.9159\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: sang, Chi-Squared: 0.0153, p-value: 0.9017\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: sit, Chi-Squared: 2.6141, p-value: 0.1059\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: 22, Chi-Squared: 4.5392, p-value: 0.0331\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: 18, Chi-Squared: 0.5380, p-value: 0.4632\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: passed, Chi-Squared: 0.2336, p-value: 0.6289\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: smash, Chi-Squared: 0.1085, p-value: 0.7418\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: film, Chi-Squared: 1.2482, p-value: 0.2639\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: tops, Chi-Squared: 0.1405, p-value: 0.7078\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: firm, Chi-Squared: 0.0075, p-value: 0.9311\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: 1847, Chi-Squared: 0.0031, p-value: 0.9557\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: community, Chi-Squared: 0.9441, p-value: 0.3312\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: working, Chi-Squared: 0.0633, p-value: 0.8014\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: breeds, Chi-Squared: 0.0042, p-value: 0.9483\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: tiny, Chi-Squared: 0.0047, p-value: 0.9454\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: potter, Chi-Squared: 0.4116, p-value: 0.5212\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: lions, Chi-Squared: 1.2232, p-value: 0.2687\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: buildings, Chi-Squared: 2.6213, p-value: 0.1054\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: trial, Chi-Squared: 0.0319, p-value: 0.8582\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: unless, Chi-Squared: 0.0196, p-value: 0.8887\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: pounds, Chi-Squared: 0.0234, p-value: 0.8784\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: mathematician, Chi-Squared: 5.6305, p-value: 0.0177\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: mom, Chi-Squared: 0.1096, p-value: 0.7406\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: malcolm, Chi-Squared: 0.0538, p-value: 0.8166\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Term: non, Chi-Squared: 9.1056, p-value: 0.0025\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import chi2_contingency\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Identify High-Frequency Words\n",
    "word_counts = Counter(\" \".join(jeopardy_df[\"clean_question\"]).split())\n",
    "threshold = 50  # Minimum occurrences to be considered\n",
    "frequent_terms = {word for word, count in word_counts.items() if count >= threshold}\n",
    "\n",
    "# Compute High/Low Value Classification\n",
    "jeopardy_df[\"high_value\"] = (jeopardy_df[\"value_usd\"] > 800).astype(int)\n",
    "\n",
    "# Efficiently Compute Word Frequencies in High/Low Value Questions\n",
    "def count_word_occurrences(word):\n",
    "    high_count = np.sum(jeopardy_df[\"high_value\"][jeopardy_df[\"clean_question\"].str.contains(f\"\\\\b{word}\\\\b\")])\n",
    "    low_count = np.sum((jeopardy_df[\"high_value\"] == 0) & (jeopardy_df[\"clean_question\"].str.contains(f\"\\\\b{word}\\\\b\")))\n",
    "    no_high = np.sum(jeopardy_df[\"high_value\"]) - high_count\n",
    "    no_low = np.sum(jeopardy_df[\"high_value\"] == 0) - low_count\n",
    "    return [[high_count, low_count], [no_high, no_low]]\n",
    "\n",
    "# Select Sample Words & Compute Observed Counts\n",
    "random.seed(777)  # Ensures reproducibility\n",
    "sorted_terms = sorted(frequent_terms)  # Convert set to a sorted list for consistent ordering\n",
    "comparison_terms = random.sample(sorted_terms, 100)\n",
    "contingency_tables = [count_word_occurrences(term) for term in comparison_terms]\n",
    "\n",
    "# Compute Chi-Squared Statistics\n",
    "chi_squared = [chi2_contingency(table)[:2] for table in contingency_tables]\n",
    "\n",
    "# Display Results\n",
    "for term, (chisq_stat, p_value) in zip(comparison_terms, chi_squared):\n",
    "    print(f\"Term: {term}, Chi-Squared: {chisq_stat:.4f}, p-value: {p_value:.4f}\", '\\n', '~'*56, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d24a4b-a12b-4c1e-a5f2-209207d2e403",
   "metadata": {},
   "source": [
    "## Analysis of Word Distribution in Jeopardy Questions  \n",
    "\n",
    "---\n",
    "\n",
    "### Methodology  \n",
    "\n",
    "To better understand how specific terms are distributed within the Jeopardy dataset, we selected 100 words at random and applied a **chi-squared test of independence**. This test helps determine if word frequencies differ significantly between the training and test sets.  \n",
    "\n",
    "**Procedure:**  \n",
    "1. **Word Selection**: Randomly selected 100 words from the dataset.  \n",
    "2. **Chi-Squared Test**: Compared each word's occurrence in the training and test sets.  \n",
    "3. **Significance Threshold**: Used a p-value of **0.05** as the cutoff for statistical significance.  \n",
    "\n",
    "**Hypotheses:**  \n",
    "- **Null Hypothesis (H₀)**: Word frequencies are consistent across the training and test sets.  \n",
    "- **Alternative Hypothesis (H₁)**: A word's frequency distribution differs significantly between the sets.  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Findings  \n",
    "\n",
    "Out of the 100 words tested, **15 exhibited statistically significant differences**, suggesting potential patterns in the dataset’s structure.  \n",
    "\n",
    "#### Words with Significant Differences (p < 0.05):  \n",
    "- **art** *(χ² = 34.33, p = 0.0000)*  \n",
    "- **i** *(χ² = 28.56, p = 0.0000)*  \n",
    "- **shows** *(χ² = 179.09, p = 0.0000)*  \n",
    "- **positive** *(χ² = 11.09, p = 0.0009)*  \n",
    "- **doomed** *(χ² = 15.93, p = 0.0001)*  \n",
    "- **baseball** *(χ² = 12.13, p = 0.0005)*  \n",
    "- **leader** *(χ² = 10.34, p = 0.0013)*  \n",
    "- **mathematician** *(χ² = 5.63, p = 0.0177)*  \n",
    "- **bergman** *(χ² = 7.14, p = 0.0075)*  \n",
    "- **titled** *(χ² = 6.94, p = 0.0084)*  \n",
    "- **isn** *(χ² = 5.33, p = 0.0210)*  \n",
    "- **street** *(χ² = 5.88, p = 0.0153)*  \n",
    "- **search** *(χ² = 4.53, p = 0.0334)*  \n",
    "- **22** *(χ² = 4.54, p = 0.0331)*  \n",
    "- **non** *(χ² = 9.11, p = 0.0025)*  \n",
    "\n",
    "---\n",
    "\n",
    "### Observations  \n",
    "\n",
    "- Words like **\"art\"**, **\"shows\"**, and **\"i\"** had particularly high chi-squared values, indicating notable distribution shifts.  \n",
    "- Proper nouns, such as **\"bergman\"** and **\"mathematician\"**, might reflect variations tied to category-specific patterns.  \n",
    "- Numeric terms like **\"22\"** and **\"non\"** could point to structural patterns related to question formatting.  \n",
    "\n",
    "---\n",
    "\n",
    "### Summary  \n",
    "\n",
    "This analysis highlights potential language patterns and structural trends within the dataset. Further investigation into these patterns could provide insights into the types of topics and terms that appear more frequently in Jeopardy questions, aiding in a deeper understanding of question design.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec332bf-a934-4f81-9090-77c98642007e",
   "metadata": {},
   "source": [
    "## Conclusion  \n",
    "\n",
    "This project analyzed patterns in the Jeopardy dataset to identify differences in word usage between the training and test sets. We applied a chi-squared test to measure how significantly word frequencies varied, uncovering potential distributional patterns.  \n",
    "\n",
    "The analysis showed that a small subset of words exhibited statistically significant differences, indicating non-random variations in their distribution. These findings provide a starting point for further exploration of language patterns in trivia contexts, though additional research is needed to confirm their broader significance.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
